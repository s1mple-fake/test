#计算机视觉第三周实验报告
##实现基于逐像素损失图像超分辨率算法和基于生成对抗网络的图像超分辨率算法，并比较两种方法的不同
###1.实验目的
- 实现SRCNN算法，对图像进行逐像素超分辨率处理，在SET5数据集上进行测试，观察超分辨率效果。
- 实现SRGAN算法，对图像进行逐像素超分辨率处理，在SET5数据集上进行测试，观察超分辨率效果。
- 从图像质量和训练过程中的不同，比较两种算法的优劣。
###2.实验原理
####2.1 SRCNN算法
SRCNN算法是一种基于深度学习的图像超分辨率算法，其核心思想是通过卷积神经网络来学习图像的低分辨率和高分辨率之间的映射关系，从而实现图像的超分辨率。SRCNN算法的网络结构如下图所示：
![SRCNN](https://pic1.zhimg.com/v2-b713ab7ee0bd8ceb4014517df9832290_b.jpg)
SRCNN算法需要一个预处理过程，将图像通过双线性插值处理，生成低分辨率图像，之后将图像输入到SRCNN网络中， 通过SRCNN中包含的三个模块(块析出与表示，非线性映射，重建)的逐步处理，得到最终的高分辨率图像。训练过程中的损失函数为MSE损失函数，即均方误差损失函数。
下侧代码块为SRCNN算法的网络结构代码：

    class SRCNN(nn.Module):

        def __init__(self, num_channels=1):
            super(SRCNN, self).__init__()
            self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=9 // 2)
            self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=5 // 2)
            self.conv3 = nn.Conv2d(32, num_channels, kernel_size=5, padding=5 // 2)
            self.relu = nn.ReLU(inplace=True)

        def forward(self, x):
            x = self.relu(self.conv1(x))
            x = self.relu(self.conv2(x))
            x = self.conv3(x)
            return x

####2.2 SRGAN算法
SRGAN算法是一种基于生成对抗网络的图像超分辨率算法，其核心思想是通过生成对抗网络来学习图像的低分辨率和高分辨率之间的映射关系，从而实现图像的超分辨率。SRGAN算法的网络结构如下图所示：
![SRGAN](https://pic1.zhimg.com/v2-261ec1c85cfbe661978f0d63543d59e8_b.jpg)
首先对图像进行四倍的下采样，之后通过GAN用于下采样图像的超分辨率重建。论文中提出，图像超分辨率的工作大都集中于以均方差（MSE）作为损失函数，这样会造成生成图像过于平滑，缺少高频纹理细节。提出了一个由adversarial loss(对抗损失)和content loss组成的损失函数，损失函数作为GAN的判别器损失函数来对生成图像进行判别。
GAN的生成器是残差块+卷积层+BN层+ReLU，对于GAN的判别器就是VGG+LeakyReLU+max-pooling，其中VGG是用来提取图像特征的，LeakyReLU是为了防止梯度消失，max-pooling是为了降低特征图的大小，使得特征图的大小和生成图像的大小一致。
下侧代码块为SRGAN算法的生成器结构代码：

    class Generator(nn.Module):
        def __init__(self, n_residual_blocks, upsample_factor):
            super(Generator, self).__init__()
            self.n_residual_blocks = n_residual_blocks
            self.upsample_factor = upsample_factor

            self.conv1 = nn.Conv2d(3, 64, 9, stride=1, padding=4)

            for i in range(self.n_residual_blocks):
                self.add_module('residual_block' + str(i+1), residualBlock())

            self.conv2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)
            self.bn2 = nn.BatchNorm2d(64)

            for i in range(self.upsample_factor//2):
                self.add_module('upsample' + str(i+1), upsampleBlock(64, 256))

            self.conv3 = nn.Conv2d(64, 3, 9, stride=1, padding=4)

        def forward(self, x):
            x = swish(self.conv1(x))

            y = x.clone()
            for i in range(self.n_residual_blocks):
                y = self.__getattr__('residual_block' + str(i+1))(y)

            x = self.bn2(self.conv2(y)) + x

            for i in range(self.upsample_factor//2):
                x = self.__getattr__('upsample' + str(i+1))(x)

            return self.conv3(x)

下侧代码块为判别器结构代码：

    class Discriminator(nn.Module):
        def __init__(self):
            super(Discriminator, self).__init__()
            self.conv1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)

            self.conv2 = nn.Conv2d(64, 64, 3, stride=2, padding=1)
            self.bn2 = nn.BatchNorm2d(64)
            self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=1)
            self.bn3 = nn.BatchNorm2d(128)
            self.conv4 = nn.Conv2d(128, 128, 3, stride=2, padding=1)
            self.bn4 = nn.BatchNorm2d(128)
            self.conv5 = nn.Conv2d(128, 256, 3, stride=1, padding=1)
            self.bn5 = nn.BatchNorm2d(256)
            self.conv6 = nn.Conv2d(256, 256, 3, stride=2, padding=1)
            self.bn6 = nn.BatchNorm2d(256)
            self.conv7 = nn.Conv2d(256, 512, 3, stride=1, padding=1)
            self.bn7 = nn.BatchNorm2d(512)
            self.conv8 = nn.Conv2d(512, 512, 3, stride=2, padding=1)
            self.bn8 = nn.BatchNorm2d(512)

            self.conv9 = nn.Conv2d(512, 1, 1, stride=1, padding=1)

        def forward(self, x):
            x = swish(self.conv1(x))

            x = swish(self.bn2(self.conv2(x)))
            x = swish(self.bn3(self.conv3(x)))
            x = swish(self.bn4(self.conv4(x)))
            x = swish(self.bn5(self.conv5(x)))
            x = swish(self.bn6(self.conv6(x)))
            x = swish(self.bn7(self.conv7(x)))
            x = swish(self.bn8(self.conv8(x)))

            x = self.conv9(x)
            return F.sigmoid(F.avg_pool2d(x, x.size()[2:])).view(x.size()[0], -1)

###实验结果分析
- 从训练过程上分析，可以从上方的代码块中看到，SRCNN的网络结构较浅，训练速度快，因此训练过程中的损失函数下降较快，但是训练过程中的损失函数下降到一定程度后，损失函数的下降速度变慢，这是由于网络结构较浅，无法提取图像的高频特征，导致生成的图像缺少高频纹理细节，从而导致损失函数的下降速度变慢。而SRGAN中使用了更深层次的网络结构，同时引入了残差连接和判别器等结构，这使得模型训练时间变得十分缓慢，但是可以注意到，SRGAN并未使用MSE损失函数，而是通过content loss和adverial loss来训练模型，这使得模型在训练过程中可以更好的提取图像的高频特征，从而使得生成的图像更加清晰，更加接近于真实图像。
- 从图像质量上分析，下面的几幅图像按顺序分别为原图、SRCNN重建图像和SRGAN重建图像。
<div align="center"> 
<img src="C:\\Users\\14394\\Desktop\\test\\week3\\data\\Set5\\butterfly.png" width = 256 height = 256 /> 
<img src="C:\\Users\\14394\\Desktop\\test\\week3\\SRCNN\\result\\butterfly_srcnn_x3.png" width = 256 height = 256 /> 
<img src="C:\\Users\\14394\\Desktop\\test\\week3\\SRGAN\\butterfly_4.png" width = 256 height = 256 /> 
</div>

<div align="center"> 
<img src="C:\\Users\\14394\\Desktop\\test\\week3\\data\\Set5\\bird.png" width = 256 height = 256 /> 
<img src="C:\\Users\\14394\\Desktop\\test\\week3\\SRCNN\\result\\bird_srcnn_x3.png" width = 256 height = 256 /> 
<img src="C:\\Users\\14394\\Desktop\\test\\week3\\SRGAN\\bird_4.png" width = 256 height = 256 /> 
</div>

<div align="center"> 
<img src="C:\\Users\\14394\\Desktop\\test\\week3\\data\\Set5\\head.png" width = 256 height = 256 /> 
<img src="C:\\Users\\14394\\Desktop\\test\\week3\\SRCNN\\result\\head_srcnn_x3.png" width = 256 height = 256 /> 
<img src="C:\\Users\\14394\\Desktop\\test\\week3\\SRGAN\\head_4.png" width = 256 height = 256 /> 
</div>

可以很明显的看到，SRCNN生成的图像缺乏高频纹理细节，对于细节的重建也不够到位1，而SRGAN生成的图像则更加清晰，更加接近于真实图像，因此可以得出结论，SRGAN生成的图像质量更好，但是训练时间更长。在时间效率要求较高的情况下，可以使用SRCNN，而在图像质量要求较高的情况下，可以使用SRGAN。